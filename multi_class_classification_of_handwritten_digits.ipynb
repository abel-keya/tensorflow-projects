{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-class_classification_of_handwritten_digits.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL",
        "266KQvZoMxMv",
        "6sfw3LH0Oycm"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ppant/tensorflow-projects/blob/master/multi_class_classification_of_handwritten_digits.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "JndnmDMp66FL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2017 Google LLC."
      ]
    },
    {
      "metadata": {
        "id": "hMqWDc_m6rUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPa95uXvcpcn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classifying Handwritten Digits with Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "Fdpn8b90u8Tp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![img](https://www.tensorflow.org/versions/r0.11/images/MNIST.png)"
      ]
    },
    {
      "metadata": {
        "id": "c7HLCm66Cs2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Learning Objectives:**\n",
        "  * Train both a linear model and a neural network to classify handwritten digits from the classic [MNIST](http://yann.lecun.com/exdb/mnist/) data set\n",
        "  * Compare the performance of the linear and neural network classification models\n",
        "  * Visualize the weights of a neural-network hidden layer"
      ]
    },
    {
      "metadata": {
        "id": "HSEh-gNdu8T0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our goal is to map each input image to the correct numeric digit. We will create a NN with a few hidden layers and a Softmax layer at the top to select the winning class."
      ]
    },
    {
      "metadata": {
        "id": "2NMdE1b-7UIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "First, let's download the data set, import TensorFlow and other utilities, and load the data into a *pandas* `DataFrame`. Note that this data is a sample of the original MNIST training data; we've taken 20000 rows at random."
      ]
    },
    {
      "metadata": {
        "id": "4LJ4SD8BWHeh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "882b227c-780d-44d5-a5e6-f05a67e7f020"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "\n",
        "mnist_dataframe = pd.read_csv(\n",
        "  \"https://dl.google.com/mlcc/mledu-datasets/mnist_train_small.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "# Use just the first 10,000 records for training/validation.\n",
        "mnist_dataframe = mnist_dataframe.head(10000)\n",
        "\n",
        "mnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
        "mnist_dataframe.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1640</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1628</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9471</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1    2    3    4    5    6    7    8    9   ...   775  776  777  \\\n",
              "1640    3    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "574     6    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "1628    2    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "958     9    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "9471    4    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "\n",
              "      778  779  780  781  782  783  784  \n",
              "1640    0    0    0    0    0    0    0  \n",
              "574     0    0    0    0    0    0    0  \n",
              "1628    0    0    0    0    0    0    0  \n",
              "958     0    0    0    0    0    0    0  \n",
              "9471    0    0    0    0    0    0    0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "kg0-25p2mOi0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each row represents one labeled example. Column 0 represents the label that a human rater has assigned for one handwritten digit. For example, if Column 0 contains '6', then a human rater interpreted the handwritten character as the digit '6'.  The ten digits 0-9 are each represented, with a unique class label for each possible digit. Thus, this is a multi-class classification problem with 10 classes."
      ]
    },
    {
      "metadata": {
        "id": "PQ7vuOwRCsZ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![img](https://www.tensorflow.org/versions/r0.11/images/MNIST-Matrix.png)"
      ]
    },
    {
      "metadata": {
        "id": "dghlqJPIu8UM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Columns 1 through 784 contain the feature values, one per pixel for the 28×28=784 pixel values. The pixel values are on a gray scale in which 0 represents white, 255 represents black, and values between 0 and 255 represent shades of gray. Most of the pixel values are 0; you may want to take a minute to confirm that they aren't all 0.  For example, adjust the following text block to print out the values in column 72."
      ]
    },
    {
      "metadata": {
        "id": "2ZkrL5MCqiJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "39cd4bfe-bbea-4005-8085-b15f764d160f"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.loc[:, 72:72]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>72</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1640</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1628</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9471</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4872</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8377</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7424</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9192</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9919</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      72\n",
              "1640   0\n",
              "574    0\n",
              "1628   0\n",
              "958    0\n",
              "9471   0\n",
              "...   ..\n",
              "4872   0\n",
              "8377   0\n",
              "7424   0\n",
              "9192   0\n",
              "9919   0\n",
              "\n",
              "[10000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "vLNg2VxqhUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's parse out the labels and features and look at a few examples. Note the use of `loc` which allows us to pull out columns based on original location, since we don't have a header row in this data set."
      ]
    },
    {
      "metadata": {
        "id": "JfFWWvMWDFrR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_labels_and_features(dataset):\n",
        "  \"\"\"Extracts labels and features.\n",
        "  \n",
        "  This is a good place to scale or transform the features if needed.\n",
        "  \n",
        "  Args:\n",
        "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
        "      monochrome pixel values on the remaining columns, in row major order.\n",
        "  Returns:\n",
        "    A `tuple` `(labels, features)`:\n",
        "      labels: A Pandas `Series`.\n",
        "      features: A Pandas `DataFrame`.\n",
        "  \"\"\"\n",
        "  labels = dataset[0]\n",
        "\n",
        "  # DataFrame.loc index ranges are inclusive at both ends.\n",
        "  features = dataset.loc[:,1:784]\n",
        "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
        "  features = features / 255\n",
        "\n",
        "  return labels, features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFY_-7vZu8UU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "c0b93102-85c7-4f5c-b25b-22273ed983ec"
      },
      "cell_type": "code",
      "source": [
        "training_targets, training_examples = parse_labels_and_features(mnist_dataframe[:7500])\n",
        "training_examples.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>...</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1      2      3      4      5      6      7      8      9      10   \\\n",
              "count 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0   \n",
              "mean     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "       ...      775    776    777    778    779    780    781    782    783  \\\n",
              "count  ...   7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0   \n",
              "mean   ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max    ...      1.0    0.8    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "         784  \n",
              "count 7500.0  \n",
              "mean     0.0  \n",
              "std      0.0  \n",
              "min      0.0  \n",
              "25%      0.0  \n",
              "50%      0.0  \n",
              "75%      0.0  \n",
              "max      0.0  \n",
              "\n",
              "[8 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "4-Vgg-1zu8Ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "085f8d56-ab2b-4d5c-8971-73fa20f296b8"
      },
      "cell_type": "code",
      "source": [
        "validation_targets, validation_examples = parse_labels_and_features(mnist_dataframe[7500:10000])\n",
        "validation_examples.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1      2      3      4      5      6      7      8      9      10   \\\n",
              "count 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0   \n",
              "mean     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "       ...      775    776    777    778    779    780    781    782    783  \\\n",
              "count  ...   2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0   \n",
              "mean   ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max    ...      1.0    1.0    0.8    0.2    1.0    0.2    0.0    0.0    0.0   \n",
              "\n",
              "         784  \n",
              "count 2500.0  \n",
              "mean     0.0  \n",
              "std      0.0  \n",
              "min      0.0  \n",
              "25%      0.0  \n",
              "50%      0.0  \n",
              "75%      0.0  \n",
              "max      0.0  \n",
              "\n",
              "[8 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "wrnAI1v6u8Uh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Show a random example and its corresponding label."
      ]
    },
    {
      "metadata": {
        "id": "s-euVJVtu8Ui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "3f429acd-4ae4-4c37-b797-f38ce55696e4"
      },
      "cell_type": "code",
      "source": [
        "rand_example = np.random.choice(training_examples.index)\n",
        "_, ax = plt.subplots()\n",
        "ax.matshow(training_examples.loc[rand_example].values.reshape(28, 28))\n",
        "ax.set_title(\"Label: %i\" % training_targets.loc[rand_example])\n",
        "ax.grid(False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFXCAYAAAAro2x+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCQAGxBIADIglABj8F8dxEe1fEvQPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdc1cd35490>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ScmYX7xdZMXE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1: Build a Linear Model for MNIST\n",
        "\n",
        "First, let's create a baseline model to compare against. The `LinearClassifier` provides a set of *k* one-vs-all classifiers, one for each of the *k* classes.\n",
        "\n",
        "You'll notice that in addition to reporting accuracy, and plotting Log Loss over time, we also display a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix).  The confusion matrix shows which classes were misclassified as other classes. Which digits get confused for each other?\n",
        "\n",
        "Also note that we track the model's error using the `log_loss` function. This should not be confused with the loss function internal to `LinearClassifier` that is used for training."
      ]
    },
    {
      "metadata": {
        "id": "cpoVC4TSdw5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def construct_feature_columns():\n",
        "  \"\"\"Construct the TensorFlow Feature Columns.\n",
        "\n",
        "  Returns:\n",
        "    A set of feature columns\n",
        "  \"\"\" \n",
        "  \n",
        "  # There are 784 pixels in each image.\n",
        "  return set([tf.feature_column.numeric_column('pixels', shape=784)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kMmL89yGeTfz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we'll make separate input functions for training and for prediction. We'll nest them in `create_training_input_fn()` and `create_predict_input_fn()`, respectively, so we can invoke these functions to return the corresponding `_input_fn`s to pass to our `.train()` and `.predict()` calls."
      ]
    },
    {
      "metadata": {
        "id": "OeS47Bmn5Ms2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_training_input_fn(features, labels, batch_size, num_epochs=None, shuffle=True):\n",
        "  \"\"\"A custom input_fn for sending MNIST data to the estimator for training.\n",
        "\n",
        "  Args:\n",
        "    features: The training features.\n",
        "    labels: The training labels.\n",
        "    batch_size: Batch size to use during training.\n",
        "\n",
        "  Returns:\n",
        "    A function that returns batches of training features and labels during\n",
        "    training.\n",
        "  \"\"\"\n",
        "  def _input_fn(num_epochs=None, shuffle=True):\n",
        "    # Input pipelines are reset with each call to .train(). To ensure model\n",
        "    # gets a good sampling of data, even when number of steps is small, we \n",
        "    # shuffle all the data before creating the Dataset object\n",
        "    idx = np.random.permutation(features.index)\n",
        "    raw_features = {\"pixels\":features.reindex(idx)}\n",
        "    raw_targets = np.array(labels[idx])\n",
        "   \n",
        "    ds = Dataset.from_tensor_slices((raw_features,raw_targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(10000)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
        "    return feature_batch, label_batch\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8zoGWAoohrwS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_predict_input_fn(features, labels, batch_size):\n",
        "  \"\"\"A custom input_fn for sending mnist data to the estimator for predictions.\n",
        "\n",
        "  Args:\n",
        "    features: The features to base predictions on.\n",
        "    labels: The labels of the prediction examples.\n",
        "\n",
        "  Returns:\n",
        "    A function that returns features and labels for predictions.\n",
        "  \"\"\"\n",
        "  def _input_fn():\n",
        "    raw_features = {\"pixels\": features.values}\n",
        "    raw_targets = np.array(labels)\n",
        "    \n",
        "    ds = Dataset.from_tensor_slices((raw_features, raw_targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size)\n",
        "    \n",
        "        \n",
        "    # Return the next batch of data.\n",
        "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
        "    return feature_batch, label_batch\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6DjSLZMu8Um",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_linear_classification_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a linear classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, and a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: An `int`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `LinearClassifier` object.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "\n",
        "  steps_per_period = steps / periods  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create a LinearClassifier object.\n",
        "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.LinearClassifier(\n",
        "      feature_columns=construct_feature_columns(),\n",
        "      n_classes=10,\n",
        "      optimizer=my_optimizer,\n",
        "      config=tf.estimator.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "  \n",
        "    # Take a break and compute probabilities.\n",
        "    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "    training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
        "        \n",
        "    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "    validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
        "    \n",
        "    # Compute training and validation errors.\n",
        "    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_errors.append(training_log_loss)\n",
        "    validation_errors.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class).\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ItHIUyv2u8Ur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Spend 5 minutes seeing how well you can do on accuracy with a linear model of this form. For this exercise, limit yourself to experimenting with the hyperparameters for batch size, learning rate and steps.**\n",
        "\n",
        "Stop if you get anything above about 0.9 accuracy."
      ]
    },
    {
      "metadata": {
        "id": "yaiIhIQqu8Uv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "9712b9ef-b061-4458-beda-71afc4e18d49"
      },
      "cell_type": "code",
      "source": [
        "classifier = train_linear_classification_model(\n",
        "             learning_rate=0.02,\n",
        "             steps=100,\n",
        "             batch_size=10,\n",
        "             training_examples=training_examples,\n",
        "             training_targets=training_targets,\n",
        "             validation_examples=validation_examples,\n",
        "             validation_targets=validation_targets)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "LogLoss error (on validation data):\n",
            "  period 00 : 16.03\n",
            "  period 01 : 11.88\n",
            "  period 02 : 9.15\n",
            "  period 03 : 7.92\n",
            "  period 04 : 7.49\n",
            "  period 05 : 7.02\n",
            "  period 06 : 6.74\n",
            "  period 07 : 6.12\n",
            "  period 08 : 6.08\n",
            "  period 09 : 6.22\n",
            "Model training finished.\n",
            "Final accuracy (on validation data): 0.82\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFnCAYAAACLnxFFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdc14e0ef50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAFnCAYAAACM3c9QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRJIwSIiISBIGCRERScIgISIiSf4f9D3Nc8wb0+MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdc141794d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "266KQvZoMxMv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "Click below for one possible solution."
      ]
    },
    {
      "metadata": {
        "id": "lRWcn24DM3qa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is a set of parameters that should attain roughly 0.9 accuracy."
      ]
    },
    {
      "metadata": {
        "id": "TGlBMrUoM1K_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_ = train_linear_classification_model(\n",
        "    learning_rate=0.03,\n",
        "    steps=1000,\n",
        "    batch_size=30,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mk095OfpPdOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2: Replace the Linear Classifier with a Neural Network\n",
        "\n",
        "**Replace the LinearClassifier above with a [`DNNClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) and find a parameter combination that gives 0.95 or better accuracy.**\n",
        "\n",
        "You may wish to experiment with additional regularization methods, such as dropout. These additional regularization methods are documented in the comments for the `DNNClassifier` class."
      ]
    },
    {
      "metadata": {
        "id": "rm8P_Ttwu8U4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# YOUR CODE HERE: Replace the linear classifier with a neural network.\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TOfmiSvqu8U9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once you have a good model, double check that you didn't overfit the validation set by evaluating on the test data that we'll load below.\n"
      ]
    },
    {
      "metadata": {
        "id": "evlB5ubzu8VJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_test_dataframe = pd.read_csv(\n",
        "  \"https://dl.google.com/mlcc/mledu-datasets/mnist_test.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
        "test_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDuLd2Hcu8VL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# YOUR CODE HERE: Calculate accuracy on the test set.\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6sfw3LH0Oycm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "Click below for a possible solution."
      ]
    },
    {
      "metadata": {
        "id": "XatDGFKEO374",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code below is almost identical to the original `LinearClassifer` training code, with the exception of the NN-specific configuration, such as the hyperparameter for hidden units."
      ]
    },
    {
      "metadata": {
        "id": "kdNTx8jkPQUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_nn_classification_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    hidden_units,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a neural network classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, as well as a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: An `int`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `DNNClassifier` object.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  # Caution: input pipelines are reset with each call to train. \n",
        "  # If the number of steps is small, your model may never see most of the data.  \n",
        "  # So with multiple `.train` calls like this you may want to control the length \n",
        "  # of training with num_epochs passed to the input_fn. Or, you can do a really-big shuffle, \n",
        "  # or since it's in-memory data, shuffle all the data in the `input_fn`.\n",
        "  steps_per_period = steps / periods  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create feature columns.\n",
        "  feature_columns = [tf.feature_column.numeric_column('pixels', shape=784)]\n",
        "\n",
        "  # Create a DNNClassifier object.\n",
        "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.DNNClassifier(\n",
        "      feature_columns=feature_columns,\n",
        "      n_classes=10,\n",
        "      hidden_units=hidden_units,\n",
        "      optimizer=my_optimizer,\n",
        "      config=tf.contrib.learn.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "  \n",
        "    # Take a break and compute probabilities.\n",
        "    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "    training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
        "        \n",
        "    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "    validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
        "    \n",
        "    # Compute training and validation errors.\n",
        "    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_errors.append(training_log_loss)\n",
        "    validation_errors.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class).\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZfzsTYGPPU8I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = train_nn_classification_model(\n",
        "    learning_rate=0.05,\n",
        "    steps=1000,\n",
        "    batch_size=30,\n",
        "    hidden_units=[100, 100],\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qXvrOgtUR-zD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we verify the accuracy on the test set."
      ]
    },
    {
      "metadata": {
        "id": "scQNpDePSFjt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_test_dataframe = pd.read_csv(\n",
        "  \"https://dl.google.com/mlcc/mledu-datasets/mnist_test.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
        "test_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EVaWpWKvSHmu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_test_input_fn = create_predict_input_fn(\n",
        "    test_examples, test_targets, batch_size=100)\n",
        "\n",
        "test_predictions = classifier.predict(input_fn=predict_test_input_fn)\n",
        "test_predictions = np.array([item['class_ids'][0] for item in test_predictions])\n",
        "  \n",
        "accuracy = metrics.accuracy_score(test_targets, test_predictions)\n",
        "print(\"Accuracy on test data: %0.2f\" % accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WX2mQBAEcisO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 3: Visualize the weights of the first hidden layer.\n",
        "\n",
        "Let's take a few minutes to dig into our neural network and see what it has learned by accessing the `weights_` attribute of our model.\n",
        "\n",
        "The input layer of our model has `784` weights corresponding to the `28×28` pixel input images. The first hidden layer will have `784×N` weights where `N` is the number of nodes in that layer. We can turn those weights back into `28×28` images by *reshaping* each of the `N` `1×784` arrays of weights into `N` arrays of size `28×28`.\n",
        "\n",
        "Run the following cell to plot the weights. Note that this cell requires that a `DNNClassifier` called \"classifier\" has already been trained."
      ]
    },
    {
      "metadata": {
        "id": "eUC0Z8nbafgG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(classifier.get_variable_names())\n",
        "\n",
        "weights0 = classifier.get_variable_value(\"dnn/hiddenlayer_0/kernel\")\n",
        "\n",
        "print(\"weights0 shape:\", weights0.shape)\n",
        "\n",
        "num_nodes = weights0.shape[1]\n",
        "num_rows = int(math.ceil(num_nodes / 10.0))\n",
        "fig, axes = plt.subplots(num_rows, 10, figsize=(20, 2 * num_rows))\n",
        "for coef, ax in zip(weights0.T, axes.ravel()):\n",
        "    # Weights in coef is reshaped from 1x784 to 28x28.\n",
        "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.pink)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kL8MEhNgrx9N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first hidden layer of the neural network should be modeling some pretty low level features, so visualizing the weights will probably just show some fuzzy blobs or possibly a few parts of digits.  You may also see some neurons that are essentially noise -- these are either unconverged or they are being ignored by higher layers.\n",
        "\n",
        "It can be interesting to stop training at different numbers of iterations and see the effect.\n",
        "\n",
        "**Train the classifier for 10, 100 and respectively 1000 steps. Then run this visualization again.**\n",
        "\n",
        "What differences do you see visually for the different levels of convergence?"
      ]
    }
  ]
}